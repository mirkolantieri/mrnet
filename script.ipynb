{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0cb51a0daa07971ff725fa0634097739836b4becdf8968cf417e3b95dc91e2288",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cb51a0daa07971ff725fa0634097739836b4becdf8968cf417e3b95dc91e2288"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#%matplotlib inline\n",
    "import random\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from IPython.display import HTML\n",
    "from skimage.io import imsave\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# Root directory for dataset\n",
    "dataroot = \"./output/\"\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Class\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator class\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create the generator\n",
    " netG = Generator(ngpu).to(device)\n",
    "\n",
    " # Handle multi-gpu if desired\n",
    " if (device.type == 'cuda') and (ngpu > 1):\n",
    "     netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)\n",
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "# For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "            # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "# Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                    % (epoch, num_epochs, i, len(dataloader),\n",
    "                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists('./gans/'):\n",
    "    os.mkdir('./gans/')\n",
    "\n",
    "for i in range(len(img_list)):\n",
    "    image0 = img_list[i]\n",
    "    save_image(image0, f'./gans/gan_{i}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import utils as ut\n",
    "from loader import MRDataset\n",
    "from model import AlexNet\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predictions(task, plane, path_to_models, train=True):\n",
    "    \"\"\" \n",
    "    `extract_predictions`: the method extracts the prediction from the pretrained models\n",
    "\n",
    "    args:\n",
    "        task: the tear to be analized (acl, meniscus, abnormal)\n",
    "        plane: the plane where the tear occured (for example: axial, coronal, sagittal)\n",
    "        path_to_models: the path where are stored the trained models\n",
    "        trains=True: boolean to indicate if the loader needs to be from the training set or the validation\n",
    "\n",
    "    \"\"\"\n",
    "    assert task in ['acl', 'meniscus', 'abnormal']\n",
    "    assert plane in ['axial', 'coronal', 'sagittal']\n",
    "    \n",
    "    # Initialize the models and filter by the tear\n",
    "    models = os.listdir(path_to_models)\n",
    "    model_name = list(filter(lambda name: task in name and plane in name, models))[0]\n",
    "    model_path = f'{path_to_models}/{model_name}'\n",
    "\n",
    "    # Select the gpu or the cpu for the tensor compilation\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else: \n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model = AlexNet()\n",
    "\n",
    "    # Load the model\n",
    "    mrnet = torch.load(model_path)\n",
    "    model.load_state_dict(mrnet)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create the traning set and send to the loader\n",
    "    dataset = MRDataset('./data/', task, plane, train=train)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2, drop_last=False)\n",
    "\n",
    "    \n",
    "    # Create the array list to store the predictions and labels from the compiled model\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    # While compiling without gradient, add each single item from the labels and prediction\n",
    "    # to the above defined array lists \n",
    "    # and then return it \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label, _ in tqdm.tqdm(loader):\n",
    "            image = image.to(device)\n",
    "            logit = model(image)\n",
    "            prediction = torch.sigmoid(logit)\n",
    "            predictions.append(prediction[0].item())\n",
    "            labels.append(label[0].item())\n",
    "    \n",
    "    return np.argmax(predictions), np.argmax(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_results_val = {}\n",
    "\n",
    "for task in ['acl', 'meniscus', 'abnormal']:\n",
    "    results = {}\n",
    "\n",
    "    # Train a logistic regressor model\n",
    "    for plane in ['axial', 'coronal', 'sagittal']:\n",
    "        prediction = extract_predictions(task, plane, './experiments/exp_opt_acc/models/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regressor for the validation test\n",
    "results_val = {}\n",
    "\n",
    "for plane in ['axial', 'coronal', 'sagittal']:\n",
    "    predictions, labels = extract_predictions(task, plane, './experiments/exp_opt_acc/models/', train=True)\n",
    "    results_val['labels'] = labels\n",
    "    results_val[plane] = predictions\n",
    "\n",
    "X_val = np.zeros((len(predictions), 3))\n",
    "X_val[:, 0] = results_val['axial']\n",
    "X_val[:, 1] = results_val['coronal']\n",
    "X_val[:, 2] = results_val['sagittal']\n",
    "\n",
    "y_val = np.array(labels)\n",
    "\n",
    "y_pred = logreg.predict_proba(X_val)[:, 1]\n",
    "y_class_preds = (y_pred > 0.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pre = pd.DataFrame(y_class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.to_csv('./pred_auc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_acc = pd.read_csv(f'./experiments/exp_opt_acc/results/complex-acl-prediction.csv')\n",
    "abnormal_acc = pd.read_csv(f'./experiments/exp_opt_acc/results/complex-abnormal-prediction.csv')\n",
    "meniscus_acc = pd.read_csv(f'./experiments/exp_opt_acc/results/complex-meniscus-prediction.csv')\n",
    "\n",
    "acl_wu = pd.read_csv(f'./experiments/exp_opt_wu/results/complex-acl-prediction.csv')\n",
    "abnormal_wu = pd.read_csv(f'./experiments/exp_opt_wu/results/complex-abnormal-prediction.csv')\n",
    "meniscus_wu = pd.read_csv(f'./experiments/exp_opt_wu/results/complex-meniscus-prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_acc = acl_acc.rename(columns={\"Unnamed: 0\": \"Case\", \"0\": \"Score\"})\n",
    "abnormal_acc = abnormal_acc.rename(columns={\"Unnamed: 0\": \"Case\", \"0\": \"Score\"})\n",
    "meniscus_acc = meniscus_acc.rename(columns={\"Unnamed: 0\": \"Case\", \"0\": \"Score\"})\n",
    "\n",
    "acl_wu = acl_wu.rename(columns={\"Unnamed: 0\": \"Case\", \"0\": \"Score\"})\n",
    "abnormal_wu = abnormal_wu.rename(columns={\"Unnamed: 0\": \"Case\", \"0\": \"Score\"})\n",
    "meniscus_wu = meniscus_wu.rename(columns={\"Unnamed: 0\": \"Case\", \"0\": \"Score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_acc[\"Prediction\"] = acl_acc['Score'] == 1\n",
    "abnormal_acc[\"Prediction\"] = abnormal_acc['Score'] == 1\n",
    "meniscus_acc[\"Prediction\"] = meniscus_acc['Score'] == 1\n",
    "\n",
    "\n",
    "acl_wu[\"Prediction\"] = acl_wu['Score'] == 1\n",
    "abnormal_wu[\"Prediction\"] = abnormal_wu['Score'] == 1\n",
    "meniscus_wu[\"Prediction\"] = meniscus_wu['Score'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_acc = acl_acc.set_index('Case')\n",
    "abnormal_acc = abnormal_acc.set_index('Case')\n",
    "meniscus_acc = meniscus_acc.set_index('Case')\n",
    "\n",
    "acl_wu = acl_wu.set_index('Case')\n",
    "abnormal_wu = abnormal_wu.set_index('Case')\n",
    "meniscus_wu = meniscus_wu.set_index('Case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acl Accuracy\n      Score  Prediction\nCase                   \n150     1.0        True\n298     1.0        True\n325     1.0        True\n------------------------------\n\nAcl WU\n      Score  Prediction\nCase                   \n150     0.0       False\n298     0.0       False\n325     0.0       False\n"
     ]
    }
   ],
   "source": [
    "print('Acl Accuracy')\n",
    "print(acl_acc.loc[acl_acc['Score'] != acl_wu['Score']])\n",
    "print('-'*30)\n",
    "print('\\nAcl WU')\n",
    "print(acl_wu.loc[acl_acc['Score'] != acl_wu['Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Abnormal Accuracy\n      Score  Prediction\nCase                   \n77      0.0       False\n230     0.0       False\n258     0.0       False\n280     0.0       False\n309     0.0       False\n------------------------------\n\nAbnormal WU\n      Score  Prediction\nCase                   \n77      1.0        True\n230     1.0        True\n258     1.0        True\n280     1.0        True\n309     1.0        True\n"
     ]
    }
   ],
   "source": [
    "print('Abnormal Accuracy')\n",
    "print(abnormal_acc.loc[abnormal_acc['Score'] != abnormal_wu['Score']])\n",
    "print('-'*30)\n",
    "print('\\nAbnormal WU')\n",
    "print(abnormal_wu.loc[abnormal_acc['Score'] != abnormal_wu['Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Meniscus Accuracy\n      Score  Prediction\nCase                   \n94      1.0        True\n97      1.0        True\n150     1.0        True\n259     1.0        True\n325     1.0        True\n------------------------------\n\nMeniscus WU\n      Score  Prediction\nCase                   \n94      0.0       False\n97      0.0       False\n150     0.0       False\n259     0.0       False\n325     0.0       False\n"
     ]
    }
   ],
   "source": [
    "print('Meniscus Accuracy')\n",
    "print(meniscus_acc.loc[meniscus_acc['Score'] != meniscus_wu['Score']])\n",
    "print('-'*30)\n",
    "print('\\nMeniscus WU')\n",
    "print(meniscus_wu.loc[meniscus_acc['Score'] != meniscus_wu['Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_acc.to_csv(f'./experiments/exp_opt_acc/results/complex-acl-prediction.csv')\n",
    "abnormal_acc.to_csv(f'./experiments/exp_opt_acc/results/complex-abnormal-prediction.csv')\n",
    "meniscus_acc.to_csv(f'./experiments/exp_opt_acc/results/complex-meniscus-prediction.csv')\n",
    "\n",
    "acl_wu.to_csv(f'./experiments/exp_opt_wu/results/complex-acl-prediction.csv')\n",
    "abnormal_wu.to_csv(f'./experiments/exp_opt_wu/results/complex-abnormal-prediction.csv')\n",
    "meniscus_wu.to_csv(f'./experiments/exp_opt_wu/results/complex-meniscus-prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f65eb29a4c0>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 372.103125 248.518125 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m4e1b7b5784\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m4e1b7b5784\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(37.369744 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.194034\" xlink:href=\"#m4e1b7b5784\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(98.242472 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.066761\" xlink:href=\"#m4e1b7b5784\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(159.115199 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.939489\" xlink:href=\"#m4e1b7b5784\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(219.987926 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.812216\" xlink:href=\"#m4e1b7b5784\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(280.860653 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.684943\" xlink:href=\"#m4e1b7b5784\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(341.733381 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1aa35687df\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1aa35687df\" y=\"214.756364\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1aa35687df\" y=\"175.221818\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 179.021037)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1aa35687df\" y=\"135.687273\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 139.486491)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1aa35687df\" y=\"96.152727\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 99.951946)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1aa35687df\" y=\"56.618182\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 60.417401)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1aa35687df\" y=\"17.083636\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#pedef220dac)\" d=\"M 45.321307 214.756364 \nL 45.321307 212.722694 \nL 45.913454 212.722694 \nL 45.913454 212.31596 \nL 46.505601 212.31596 \nL 46.505601 211.502492 \nL 47.097748 211.502492 \nL 47.097748 210.689024 \nL 47.689895 210.689024 \nL 47.689895 209.875556 \nL 48.282043 209.875556 \nL 48.282043 208.655354 \nL 48.87419 208.655354 \nL 48.87419 208.24862 \nL 50.058484 208.24862 \nL 50.058484 207.841886 \nL 50.650631 207.841886 \nL 50.650631 207.435152 \nL 51.834925 207.435152 \nL 51.834925 207.028418 \nL 52.427073 207.028418 \nL 52.427073 206.621684 \nL 53.01922 206.621684 \nL 53.01922 205.401481 \nL 54.203514 205.401481 \nL 54.203514 204.994747 \nL 55.387808 204.994747 \nL 55.387808 204.588013 \nL 55.979956 204.588013 \nL 55.979956 202.961077 \nL 57.16425 202.961077 \nL 57.16425 202.554343 \nL 57.756397 202.554343 \nL 57.756397 201.334141 \nL 60.124986 201.334141 \nL 60.124986 200.520673 \nL 61.901427 200.520673 \nL 61.901427 200.113939 \nL 63.677869 200.113939 \nL 63.677869 199.300471 \nL 64.862163 199.300471 \nL 64.862163 198.080269 \nL 67.822899 198.080269 \nL 67.822899 197.266801 \nL 69.007193 197.266801 \nL 69.007193 195.233131 \nL 69.59934 195.233131 \nL 69.59934 194.826397 \nL 71.375782 194.826397 \nL 71.375782 194.012929 \nL 73.152223 194.012929 \nL 73.152223 193.606195 \nL 73.74437 193.606195 \nL 73.74437 193.199461 \nL 74.928664 193.199461 \nL 74.928664 191.979259 \nL 76.112959 191.979259 \nL 76.112959 189.945589 \nL 77.8894 189.945589 \nL 77.8894 188.318653 \nL 79.665842 188.318653 \nL 79.665842 187.505185 \nL 80.257989 187.505185 \nL 80.257989 187.098451 \nL 81.442283 187.098451 \nL 81.442283 185.878249 \nL 82.03443 185.878249 \nL 82.03443 185.471515 \nL 82.626577 185.471515 \nL 82.626577 185.064781 \nL 86.17946 185.064781 \nL 86.17946 183.844579 \nL 86.771607 183.844579 \nL 86.771607 183.031111 \nL 87.363755 183.031111 \nL 87.363755 181.404175 \nL 88.548049 181.404175 \nL 88.548049 180.997441 \nL 89.140196 180.997441 \nL 89.140196 180.590707 \nL 89.732343 180.590707 \nL 89.732343 180.183973 \nL 91.508785 180.183973 \nL 91.508785 179.777239 \nL 93.285226 179.777239 \nL 93.285226 179.370505 \nL 94.46952 179.370505 \nL 94.46952 178.963771 \nL 95.061668 178.963771 \nL 95.061668 178.557037 \nL 95.653815 178.557037 \nL 95.653815 178.150303 \nL 96.245962 178.150303 \nL 96.245962 176.523367 \nL 99.206698 176.523367 \nL 99.206698 176.116633 \nL 99.798845 176.116633 \nL 99.798845 175.709899 \nL 102.167433 175.709899 \nL 102.167433 175.303165 \nL 102.759581 175.303165 \nL 102.759581 173.269495 \nL 103.351728 173.269495 \nL 103.351728 172.862761 \nL 104.536022 172.862761 \nL 104.536022 172.049293 \nL 105.128169 172.049293 \nL 105.128169 171.235825 \nL 106.904611 171.235825 \nL 106.904611 169.608889 \nL 108.088905 169.608889 \nL 108.088905 167.981953 \nL 109.273199 167.981953 \nL 109.273199 167.575219 \nL 109.865346 167.575219 \nL 109.865346 166.761751 \nL 112.233935 166.761751 \nL 112.233935 165.948283 \nL 113.418229 165.948283 \nL 113.418229 164.321347 \nL 114.602524 164.321347 \nL 114.602524 163.914613 \nL 116.378965 163.914613 \nL 116.378965 163.507879 \nL 117.563259 163.507879 \nL 117.563259 163.101145 \nL 118.155407 163.101145 \nL 118.155407 162.287677 \nL 118.747554 162.287677 \nL 118.747554 161.880943 \nL 119.339701 161.880943 \nL 119.339701 161.474209 \nL 120.523995 161.474209 \nL 120.523995 161.067475 \nL 121.116142 161.067475 \nL 121.116142 160.254007 \nL 121.708289 160.254007 \nL 121.708289 159.847273 \nL 124.076878 159.847273 \nL 124.076878 159.033805 \nL 125.85332 159.033805 \nL 125.85332 158.627071 \nL 127.037614 158.627071 \nL 127.037614 158.220337 \nL 129.406202 158.220337 \nL 129.406202 157.406869 \nL 131.774791 157.406869 \nL 131.774791 155.779933 \nL 132.366938 155.779933 \nL 132.366938 154.559731 \nL 132.959085 154.559731 \nL 132.959085 153.339529 \nL 134.14338 153.339529 \nL 134.14338 152.526061 \nL 135.327674 152.526061 \nL 135.327674 151.305859 \nL 135.919821 151.305859 \nL 135.919821 150.492391 \nL 136.511968 150.492391 \nL 136.511968 150.085657 \nL 137.104115 150.085657 \nL 137.104115 149.678923 \nL 137.696263 149.678923 \nL 137.696263 149.272189 \nL 138.28841 149.272189 \nL 138.28841 148.865455 \nL 140.064851 148.865455 \nL 140.064851 148.458721 \nL 140.656998 148.458721 \nL 140.656998 148.051987 \nL 142.43344 148.051987 \nL 142.43344 147.645253 \nL 143.025587 147.645253 \nL 143.025587 146.425051 \nL 144.209881 146.425051 \nL 144.209881 146.018316 \nL 145.394176 146.018316 \nL 145.394176 144.798114 \nL 145.986323 144.798114 \nL 145.986323 143.577912 \nL 146.57847 143.577912 \nL 146.57847 143.171178 \nL 147.170617 143.171178 \nL 147.170617 142.764444 \nL 148.947058 142.764444 \nL 148.947058 142.35771 \nL 149.539206 142.35771 \nL 149.539206 141.950976 \nL 150.131353 141.950976 \nL 150.131353 141.544242 \nL 150.7235 141.544242 \nL 150.7235 141.137508 \nL 151.907794 141.137508 \nL 151.907794 140.730774 \nL 157.237119 140.730774 \nL 157.237119 140.32404 \nL 157.829266 140.32404 \nL 157.829266 139.917306 \nL 159.01356 139.917306 \nL 159.01356 138.29037 \nL 159.605707 138.29037 \nL 159.605707 137.883636 \nL 160.197854 137.883636 \nL 160.197854 137.070168 \nL 162.566443 137.070168 \nL 162.566443 136.2567 \nL 163.15859 136.2567 \nL 163.15859 135.849966 \nL 164.342884 135.849966 \nL 164.342884 135.443232 \nL 164.935032 135.443232 \nL 164.935032 135.036498 \nL 165.527179 135.036498 \nL 165.527179 134.629764 \nL 166.119326 134.629764 \nL 166.119326 133.002828 \nL 168.487915 133.002828 \nL 168.487915 132.596094 \nL 170.264356 132.596094 \nL 170.264356 132.18936 \nL 170.856503 132.18936 \nL 170.856503 131.782626 \nL 171.44865 131.782626 \nL 171.44865 130.969158 \nL 172.040797 130.969158 \nL 172.040797 130.562424 \nL 176.185828 130.562424 \nL 176.185828 130.15569 \nL 176.777975 130.15569 \nL 176.777975 129.748956 \nL 177.370122 129.748956 \nL 177.370122 128.528754 \nL 178.554416 128.528754 \nL 178.554416 127.715286 \nL 179.146563 127.715286 \nL 179.146563 127.308552 \nL 179.73871 127.308552 \nL 179.73871 126.901818 \nL 180.923005 126.901818 \nL 180.923005 126.495084 \nL 181.515152 126.495084 \nL 181.515152 126.08835 \nL 185.068035 126.08835 \nL 185.068035 125.681616 \nL 186.844476 125.681616 \nL 186.844476 124.05468 \nL 188.028771 124.05468 \nL 188.028771 123.647946 \nL 188.620918 123.647946 \nL 188.620918 123.241212 \nL 189.213065 123.241212 \nL 189.213065 122.834478 \nL 190.397359 122.834478 \nL 190.397359 122.02101 \nL 190.989506 122.02101 \nL 190.989506 120.800808 \nL 192.173801 120.800808 \nL 192.173801 119.173872 \nL 192.765948 119.173872 \nL 192.765948 118.767138 \nL 193.358095 118.767138 \nL 193.358095 118.360404 \nL 193.950242 118.360404 \nL 193.950242 117.546936 \nL 194.542389 117.546936 \nL 194.542389 117.140202 \nL 195.134536 117.140202 \nL 195.134536 116.733468 \nL 195.726684 116.733468 \nL 195.726684 116.326734 \nL 196.318831 116.326734 \nL 196.318831 115.92 \nL 196.910978 115.92 \nL 196.910978 114.293064 \nL 199.279566 114.293064 \nL 199.279566 113.479596 \nL 199.871714 113.479596 \nL 199.871714 111.039192 \nL 200.463861 111.039192 \nL 200.463861 110.632458 \nL 201.056008 110.632458 \nL 201.056008 110.225724 \nL 202.832449 110.225724 \nL 202.832449 107.78532 \nL 203.424597 107.78532 \nL 203.424597 107.378586 \nL 204.016744 107.378586 \nL 204.016744 106.565118 \nL 204.608891 106.565118 \nL 204.608891 105.344916 \nL 206.385332 105.344916 \nL 206.385332 104.124714 \nL 206.977479 104.124714 \nL 206.977479 102.904512 \nL 208.161774 102.904512 \nL 208.161774 102.497778 \nL 208.753921 102.497778 \nL 208.753921 101.68431 \nL 209.346068 101.68431 \nL 209.346068 100.870842 \nL 211.12251 100.870842 \nL 211.12251 100.057374 \nL 212.306804 100.057374 \nL 212.306804 99.243906 \nL 213.491098 99.243906 \nL 213.491098 98.430438 \nL 215.26754 98.430438 \nL 215.26754 98.023704 \nL 216.451834 98.023704 \nL 216.451834 96.803502 \nL 217.043981 96.803502 \nL 217.043981 96.396768 \nL 218.820422 96.396768 \nL 218.820422 95.990034 \nL 220.004717 95.990034 \nL 220.004717 95.5833 \nL 221.189011 95.5833 \nL 221.189011 95.176566 \nL 222.373305 95.176566 \nL 222.373305 94.769832 \nL 222.965453 94.769832 \nL 222.965453 94.363098 \nL 223.5576 94.363098 \nL 223.5576 93.956364 \nL 225.334041 93.956364 \nL 225.334041 93.142896 \nL 226.518335 93.142896 \nL 226.518335 92.736162 \nL 227.110483 92.736162 \nL 227.110483 91.922694 \nL 227.70263 91.922694 \nL 227.70263 91.109226 \nL 228.294777 91.109226 \nL 228.294777 90.702492 \nL 229.479071 90.702492 \nL 229.479071 89.48229 \nL 230.663366 89.48229 \nL 230.663366 89.075556 \nL 231.255513 89.075556 \nL 231.255513 88.668822 \nL 231.84766 88.668822 \nL 231.84766 84.601481 \nL 233.031954 84.601481 \nL 233.031954 83.788013 \nL 233.624101 83.788013 \nL 233.624101 82.974545 \nL 235.99269 82.974545 \nL 235.99269 80.127407 \nL 236.584837 80.127407 \nL 236.584837 79.313939 \nL 237.176984 79.313939 \nL 237.176984 78.093737 \nL 238.361279 78.093737 \nL 238.361279 77.280269 \nL 239.545573 77.280269 \nL 239.545573 76.873535 \nL 240.729867 76.873535 \nL 240.729867 76.466801 \nL 241.322014 76.466801 \nL 241.322014 76.060067 \nL 242.506309 76.060067 \nL 242.506309 74.433131 \nL 243.098456 74.433131 \nL 243.098456 72.806195 \nL 244.28275 72.806195 \nL 244.28275 71.992727 \nL 246.059192 71.992727 \nL 246.059192 71.585993 \nL 246.651339 71.585993 \nL 246.651339 67.518653 \nL 247.243486 67.518653 \nL 247.243486 67.111919 \nL 247.835633 67.111919 \nL 247.835633 66.705185 \nL 249.019927 66.705185 \nL 249.019927 66.298451 \nL 249.612074 66.298451 \nL 249.612074 65.891717 \nL 250.204222 65.891717 \nL 250.204222 65.484983 \nL 251.388516 65.484983 \nL 251.388516 65.078249 \nL 253.164957 65.078249 \nL 253.164957 63.858047 \nL 254.941399 63.858047 \nL 254.941399 63.451313 \nL 256.125693 63.451313 \nL 256.125693 63.044579 \nL 257.309987 63.044579 \nL 257.309987 62.637845 \nL 259.678576 62.637845 \nL 259.678576 62.231111 \nL 260.270723 62.231111 \nL 260.270723 61.010909 \nL 260.86287 61.010909 \nL 260.86287 60.604175 \nL 263.231459 60.604175 \nL 263.231459 60.197441 \nL 263.823606 60.197441 \nL 263.823606 59.383973 \nL 264.415753 59.383973 \nL 264.415753 58.570505 \nL 265.0079 58.570505 \nL 265.0079 57.757037 \nL 265.600048 57.757037 \nL 265.600048 57.350303 \nL 267.376489 57.350303 \nL 267.376489 56.943569 \nL 270.337225 56.943569 \nL 270.337225 56.130101 \nL 270.929372 56.130101 \nL 270.929372 55.723367 \nL 271.521519 55.723367 \nL 271.521519 55.316633 \nL 272.113666 55.316633 \nL 272.113666 54.909899 \nL 274.482255 54.909899 \nL 274.482255 54.503165 \nL 275.074402 54.503165 \nL 275.074402 54.096431 \nL 276.850843 54.096431 \nL 276.850843 53.689697 \nL 278.627285 53.689697 \nL 278.627285 53.282963 \nL 279.219432 53.282963 \nL 279.219432 51.656027 \nL 279.811579 51.656027 \nL 279.811579 51.249293 \nL 280.995873 51.249293 \nL 280.995873 50.435825 \nL 282.180168 50.435825 \nL 282.180168 50.029091 \nL 283.956609 50.029091 \nL 283.956609 49.622357 \nL 285.733051 49.622357 \nL 285.733051 49.215623 \nL 286.917345 49.215623 \nL 286.917345 48.808889 \nL 288.693786 48.808889 \nL 288.693786 47.995421 \nL 289.878081 47.995421 \nL 289.878081 47.181953 \nL 290.470228 47.181953 \nL 290.470228 46.775219 \nL 291.062375 46.775219 \nL 291.062375 45.961751 \nL 292.246669 45.961751 \nL 292.246669 45.555017 \nL 294.023111 45.555017 \nL 294.023111 45.148283 \nL 295.207405 45.148283 \nL 295.207405 44.334815 \nL 295.799552 44.334815 \nL 295.799552 43.928081 \nL 296.983847 43.928081 \nL 296.983847 42.707879 \nL 302.313171 42.707879 \nL 302.313171 41.487677 \nL 302.905318 41.487677 \nL 302.905318 41.080943 \nL 303.497465 41.080943 \nL 303.497465 40.674209 \nL 305.273907 40.674209 \nL 305.273907 39.860741 \nL 308.234643 39.860741 \nL 308.234643 38.640539 \nL 309.418937 38.640539 \nL 309.418937 38.233805 \nL 310.011084 38.233805 \nL 310.011084 37.827071 \nL 310.603231 37.827071 \nL 310.603231 37.420337 \nL 311.195378 37.420337 \nL 311.195378 37.013603 \nL 312.379673 37.013603 \nL 312.379673 35.793401 \nL 315.932555 35.793401 \nL 315.932555 35.386667 \nL 317.708997 35.386667 \nL 317.708997 34.166465 \nL 318.893291 34.166465 \nL 318.893291 33.352997 \nL 320.077586 33.352997 \nL 320.077586 32.946263 \nL 321.26188 32.946263 \nL 321.26188 32.132795 \nL 321.854027 32.132795 \nL 321.854027 31.726061 \nL 322.446174 31.726061 \nL 322.446174 31.319327 \nL 323.038321 31.319327 \nL 323.038321 30.099125 \nL 326.591204 30.099125 \nL 326.591204 29.692391 \nL 328.367646 29.692391 \nL 328.367646 27.251987 \nL 328.959793 27.251987 \nL 328.959793 26.845253 \nL 330.144087 26.845253 \nL 330.144087 26.438519 \nL 331.920529 26.438519 \nL 331.920529 26.031785 \nL 332.512676 26.031785 \nL 332.512676 25.218316 \nL 333.104823 25.218316 \nL 333.104823 24.404848 \nL 333.69697 24.404848 \nL 333.69697 23.998114 \nL 334.289117 23.998114 \nL 334.289117 22.777912 \nL 336.657706 22.777912 \nL 336.657706 22.371178 \nL 339.026294 22.371178 \nL 339.026294 21.964444 \nL 340.802736 21.964444 \nL 340.802736 19.930774 \nL 343.171325 19.930774 \nL 343.171325 18.710572 \nL 345.539913 18.710572 \nL 345.539913 18.303838 \nL 347.316355 18.303838 \nL 347.316355 17.897104 \nL 348.500649 17.897104 \nL 348.500649 17.083636 \nL 349.684943 17.083636 \nL 349.684943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#pedef220dac)\" d=\"M 45.321307 214.756364 \nL 47.90066 214.756364 \nL 47.90066 214.007603 \nL 50.480013 214.007603 \nL 50.480013 212.135702 \nL 51.124851 212.135702 \nL 51.124851 211.761322 \nL 52.414527 211.761322 \nL 52.414527 211.012562 \nL 53.059365 211.012562 \nL 53.059365 210.638182 \nL 53.704204 210.638182 \nL 53.704204 209.889421 \nL 54.349042 209.889421 \nL 54.349042 209.515041 \nL 54.99388 209.515041 \nL 54.99388 208.391901 \nL 55.638718 208.391901 \nL 55.638718 206.52 \nL 56.928395 206.52 \nL 56.928395 206.14562 \nL 59.507747 206.14562 \nL 59.507747 205.39686 \nL 60.797424 205.39686 \nL 60.797424 204.273719 \nL 61.442262 204.273719 \nL 61.442262 203.524959 \nL 62.0871 203.524959 \nL 62.0871 203.150579 \nL 62.731939 203.150579 \nL 62.731939 202.776198 \nL 64.021615 202.776198 \nL 64.021615 202.401818 \nL 64.666453 202.401818 \nL 64.666453 202.027438 \nL 65.95613 202.027438 \nL 65.95613 201.653058 \nL 67.245806 201.653058 \nL 67.245806 201.278678 \nL 67.890644 201.278678 \nL 67.890644 200.529917 \nL 68.535482 200.529917 \nL 68.535482 199.781157 \nL 69.825159 199.781157 \nL 69.825159 198.658017 \nL 71.114835 198.658017 \nL 71.114835 197.909256 \nL 71.759674 197.909256 \nL 71.759674 197.160496 \nL 73.04935 197.160496 \nL 73.04935 196.411736 \nL 74.339026 196.411736 \nL 74.339026 196.037355 \nL 74.983865 196.037355 \nL 74.983865 195.288595 \nL 75.628703 195.288595 \nL 75.628703 194.914215 \nL 76.273541 194.914215 \nL 76.273541 194.539835 \nL 77.563217 194.539835 \nL 77.563217 193.791074 \nL 79.497732 193.791074 \nL 79.497732 193.042314 \nL 80.14257 193.042314 \nL 80.14257 192.667934 \nL 81.432247 192.667934 \nL 81.432247 191.919174 \nL 82.077085 191.919174 \nL 82.077085 189.672893 \nL 82.721923 189.672893 \nL 82.721923 188.175372 \nL 83.366761 188.175372 \nL 83.366761 187.800992 \nL 85.946114 187.800992 \nL 85.946114 184.80595 \nL 87.235791 184.80595 \nL 87.235791 184.43157 \nL 87.880629 184.43157 \nL 87.880629 183.68281 \nL 89.170305 183.68281 \nL 89.170305 182.559669 \nL 89.815143 182.559669 \nL 89.815143 181.810909 \nL 90.459982 181.810909 \nL 90.459982 181.436529 \nL 91.10482 181.436529 \nL 91.10482 180.687769 \nL 91.749658 180.687769 \nL 91.749658 180.313388 \nL 92.394496 180.313388 \nL 92.394496 179.939008 \nL 93.684173 179.939008 \nL 93.684173 179.190248 \nL 94.329011 179.190248 \nL 94.329011 178.815868 \nL 95.618687 178.815868 \nL 95.618687 178.441488 \nL 96.263526 178.441488 \nL 96.263526 177.692727 \nL 97.553202 177.692727 \nL 97.553202 177.318347 \nL 98.19804 177.318347 \nL 98.19804 175.820826 \nL 98.842878 175.820826 \nL 98.842878 175.072066 \nL 99.487717 175.072066 \nL 99.487717 173.948926 \nL 100.132555 173.948926 \nL 100.132555 172.077025 \nL 100.777393 172.077025 \nL 100.777393 171.328264 \nL 101.422231 171.328264 \nL 101.422231 170.953884 \nL 102.711908 170.953884 \nL 102.711908 169.830744 \nL 103.356746 169.830744 \nL 103.356746 168.333223 \nL 104.646422 168.333223 \nL 104.646422 167.584463 \nL 105.291261 167.584463 \nL 105.291261 167.210083 \nL 105.936099 167.210083 \nL 105.936099 166.461322 \nL 107.225775 166.461322 \nL 107.225775 166.086942 \nL 107.870613 166.086942 \nL 107.870613 165.338182 \nL 109.16029 165.338182 \nL 109.16029 164.963802 \nL 110.449966 164.963802 \nL 110.449966 164.215041 \nL 111.094805 164.215041 \nL 111.094805 161.59438 \nL 111.739643 161.59438 \nL 111.739643 161.22 \nL 112.384481 161.22 \nL 112.384481 160.84562 \nL 114.318996 160.84562 \nL 114.318996 160.47124 \nL 115.608672 160.47124 \nL 115.608672 160.09686 \nL 117.543187 160.09686 \nL 117.543187 159.722479 \nL 118.188025 159.722479 \nL 118.188025 158.973719 \nL 119.477701 158.973719 \nL 119.477701 158.224959 \nL 123.346731 158.224959 \nL 123.346731 157.850579 \nL 123.991569 157.850579 \nL 123.991569 156.727438 \nL 125.281245 156.727438 \nL 125.281245 155.978678 \nL 126.570922 155.978678 \nL 126.570922 155.604298 \nL 127.21576 155.604298 \nL 127.21576 155.229917 \nL 127.860598 155.229917 \nL 127.860598 153.732397 \nL 131.084789 153.732397 \nL 131.084789 153.358017 \nL 131.729627 153.358017 \nL 131.729627 152.983636 \nL 132.374466 152.983636 \nL 132.374466 152.609256 \nL 133.019304 152.609256 \nL 133.019304 152.234876 \nL 133.664142 152.234876 \nL 133.664142 151.860496 \nL 134.30898 151.860496 \nL 134.30898 151.486116 \nL 134.953818 151.486116 \nL 134.953818 151.111736 \nL 135.598657 151.111736 \nL 135.598657 150.362975 \nL 136.888333 150.362975 \nL 136.888333 148.491074 \nL 138.178009 148.491074 \nL 138.178009 147.742314 \nL 138.822848 147.742314 \nL 138.822848 147.367934 \nL 140.112524 147.367934 \nL 140.112524 146.993554 \nL 140.757362 146.993554 \nL 140.757362 146.619174 \nL 142.691877 146.619174 \nL 142.691877 145.870413 \nL 144.626392 145.870413 \nL 144.626392 144.747273 \nL 145.27123 144.747273 \nL 145.27123 144.372893 \nL 147.205744 144.372893 \nL 147.205744 143.998512 \nL 147.850583 143.998512 \nL 147.850583 143.249752 \nL 149.140259 143.249752 \nL 149.140259 141.752231 \nL 151.074774 141.752231 \nL 151.074774 140.629091 \nL 151.719612 140.629091 \nL 151.719612 139.880331 \nL 153.009288 139.880331 \nL 153.009288 139.50595 \nL 153.654127 139.50595 \nL 153.654127 138.75719 \nL 154.943803 138.75719 \nL 154.943803 137.63405 \nL 155.588641 137.63405 \nL 155.588641 135.387769 \nL 157.523156 135.387769 \nL 157.523156 135.013388 \nL 158.167994 135.013388 \nL 158.167994 134.264628 \nL 159.45767 134.264628 \nL 159.45767 133.890248 \nL 160.102509 133.890248 \nL 160.102509 132.392727 \nL 160.747347 132.392727 \nL 160.747347 130.146446 \nL 162.681862 130.146446 \nL 162.681862 129.023306 \nL 163.971538 129.023306 \nL 163.971538 128.648926 \nL 164.616376 128.648926 \nL 164.616376 128.274545 \nL 167.840567 128.274545 \nL 167.840567 126.777025 \nL 168.485405 126.777025 \nL 168.485405 124.905124 \nL 169.130244 124.905124 \nL 169.130244 123.781983 \nL 169.775082 123.781983 \nL 169.775082 123.407603 \nL 171.064758 123.407603 \nL 171.064758 122.658843 \nL 171.709596 122.658843 \nL 171.709596 121.910083 \nL 172.354435 121.910083 \nL 172.354435 121.535702 \nL 173.644111 121.535702 \nL 173.644111 121.161322 \nL 174.288949 121.161322 \nL 174.288949 120.786942 \nL 174.933788 120.786942 \nL 174.933788 120.412562 \nL 177.51314 120.412562 \nL 177.51314 120.038182 \nL 180.092493 120.038182 \nL 180.092493 119.289421 \nL 182.671846 119.289421 \nL 182.671846 118.166281 \nL 183.961523 118.166281 \nL 183.961523 117.791901 \nL 185.896037 117.791901 \nL 185.896037 117.417521 \nL 187.185714 117.417521 \nL 187.185714 117.04314 \nL 187.830552 117.04314 \nL 187.830552 116.29438 \nL 190.409905 116.29438 \nL 190.409905 115.54562 \nL 191.054743 115.54562 \nL 191.054743 114.422479 \nL 192.344419 114.422479 \nL 192.344419 113.299339 \nL 193.634096 113.299339 \nL 193.634096 112.924959 \nL 194.278934 112.924959 \nL 194.278934 112.550579 \nL 195.56861 112.550579 \nL 195.56861 111.801818 \nL 196.213449 111.801818 \nL 196.213449 110.678678 \nL 196.858287 110.678678 \nL 196.858287 110.304298 \nL 197.503125 110.304298 \nL 197.503125 109.929917 \nL 199.43764 109.929917 \nL 199.43764 108.806777 \nL 200.082478 108.806777 \nL 200.082478 108.432397 \nL 201.372154 108.432397 \nL 201.372154 108.058017 \nL 203.306669 108.058017 \nL 203.306669 107.683636 \nL 204.596345 107.683636 \nL 204.596345 106.934876 \nL 205.241184 106.934876 \nL 205.241184 106.186116 \nL 206.53086 106.186116 \nL 206.53086 105.062975 \nL 209.110213 105.062975 \nL 209.110213 104.314215 \nL 209.755051 104.314215 \nL 209.755051 103.565455 \nL 211.044727 103.565455 \nL 211.044727 102.816694 \nL 211.689566 102.816694 \nL 211.689566 102.067934 \nL 212.979242 102.067934 \nL 212.979242 100.944793 \nL 214.913757 100.944793 \nL 214.913757 100.570413 \nL 215.558595 100.570413 \nL 215.558595 100.196033 \nL 218.782786 100.196033 \nL 218.782786 99.072893 \nL 219.427624 99.072893 \nL 219.427624 98.698512 \nL 220.072462 98.698512 \nL 220.072462 97.200992 \nL 220.717301 97.200992 \nL 220.717301 96.826612 \nL 221.362139 96.826612 \nL 221.362139 96.077851 \nL 222.651815 96.077851 \nL 222.651815 95.329091 \nL 225.231168 95.329091 \nL 225.231168 94.954711 \nL 226.520845 94.954711 \nL 226.520845 94.580331 \nL 227.165683 94.580331 \nL 227.165683 93.45719 \nL 227.810521 93.45719 \nL 227.810521 93.08281 \nL 229.100197 93.08281 \nL 229.100197 91.585289 \nL 232.324388 91.585289 \nL 232.324388 91.210909 \nL 232.969227 91.210909 \nL 232.969227 90.836529 \nL 234.258903 90.836529 \nL 234.258903 90.087769 \nL 235.54858 90.087769 \nL 235.54858 89.713388 \nL 236.838256 89.713388 \nL 236.838256 88.590248 \nL 238.127932 88.590248 \nL 238.127932 88.215868 \nL 239.417609 88.215868 \nL 239.417609 85.969587 \nL 241.996962 85.969587 \nL 241.996962 85.595207 \nL 243.286638 85.595207 \nL 243.286638 84.846446 \nL 244.576315 84.846446 \nL 244.576315 84.097686 \nL 248.445344 84.097686 \nL 248.445344 83.723306 \nL 249.090182 83.723306 \nL 249.090182 82.974545 \nL 250.379858 82.974545 \nL 250.379858 82.225785 \nL 252.959211 82.225785 \nL 252.959211 80.728264 \nL 254.893726 80.728264 \nL 254.893726 80.353884 \nL 256.183402 80.353884 \nL 256.183402 79.605124 \nL 256.828241 79.605124 \nL 256.828241 78.481983 \nL 257.473079 78.481983 \nL 257.473079 77.358843 \nL 260.69727 77.358843 \nL 260.69727 76.610083 \nL 261.342108 76.610083 \nL 261.342108 75.861322 \nL 265.211137 75.861322 \nL 265.211137 74.363802 \nL 266.500814 74.363802 \nL 266.500814 73.989421 \nL 267.79049 73.989421 \nL 267.79049 72.117521 \nL 268.435328 72.117521 \nL 268.435328 70.62 \nL 269.080167 70.62 \nL 269.080167 70.24562 \nL 270.369843 70.24562 \nL 270.369843 69.87124 \nL 271.659519 69.87124 \nL 271.659519 69.122479 \nL 272.949196 69.122479 \nL 272.949196 67.999339 \nL 274.238872 67.999339 \nL 274.238872 67.624959 \nL 275.528549 67.624959 \nL 275.528549 66.876198 \nL 276.173387 66.876198 \nL 276.173387 66.127438 \nL 276.818225 66.127438 \nL 276.818225 65.378678 \nL 277.463063 65.378678 \nL 277.463063 65.004298 \nL 278.75274 65.004298 \nL 278.75274 63.506777 \nL 279.397578 63.506777 \nL 279.397578 63.132397 \nL 281.976931 63.132397 \nL 281.976931 62.758017 \nL 282.621769 62.758017 \nL 282.621769 62.383636 \nL 283.911445 62.383636 \nL 283.911445 62.009256 \nL 284.556284 62.009256 \nL 284.556284 61.260496 \nL 285.201122 61.260496 \nL 285.201122 60.886116 \nL 285.84596 60.886116 \nL 285.84596 59.762975 \nL 288.425313 59.762975 \nL 288.425313 59.388595 \nL 291.004666 59.388595 \nL 291.004666 59.014215 \nL 292.294342 59.014215 \nL 292.294342 58.639835 \nL 293.584019 58.639835 \nL 293.584019 58.265455 \nL 294.228857 58.265455 \nL 294.228857 57.516694 \nL 295.518533 57.516694 \nL 295.518533 56.393554 \nL 296.163372 56.393554 \nL 296.163372 56.019174 \nL 297.453048 56.019174 \nL 297.453048 54.896033 \nL 298.097886 54.896033 \nL 298.097886 54.147273 \nL 298.742724 54.147273 \nL 298.742724 53.398512 \nL 299.387563 53.398512 \nL 299.387563 52.649752 \nL 300.032401 52.649752 \nL 300.032401 52.275372 \nL 300.677239 52.275372 \nL 300.677239 51.900992 \nL 302.611754 51.900992 \nL 302.611754 48.53157 \nL 303.256592 48.53157 \nL 303.256592 47.78281 \nL 304.546268 47.78281 \nL 304.546268 45.162149 \nL 305.191107 45.162149 \nL 305.191107 44.413388 \nL 306.480783 44.413388 \nL 306.480783 43.664628 \nL 307.125621 43.664628 \nL 307.125621 42.541488 \nL 309.060136 42.541488 \nL 309.060136 41.792727 \nL 309.704974 41.792727 \nL 309.704974 41.043967 \nL 311.639489 41.043967 \nL 311.639489 40.669587 \nL 313.574003 40.669587 \nL 313.574003 40.295207 \nL 318.087871 40.295207 \nL 318.087871 39.920826 \nL 318.732709 39.920826 \nL 318.732709 37.674545 \nL 320.667224 37.674545 \nL 320.667224 37.300165 \nL 322.601738 37.300165 \nL 322.601738 36.551405 \nL 323.246576 36.551405 \nL 323.246576 36.177025 \nL 324.536253 36.177025 \nL 324.536253 33.181983 \nL 327.115606 33.181983 \nL 327.115606 32.433223 \nL 328.405282 32.433223 \nL 328.405282 32.058843 \nL 329.05012 32.058843 \nL 329.05012 31.684463 \nL 329.694959 31.684463 \nL 329.694959 31.310083 \nL 330.339797 31.310083 \nL 330.339797 30.186942 \nL 330.984635 30.186942 \nL 330.984635 29.438182 \nL 332.91915 29.438182 \nL 332.91915 28.315041 \nL 333.563988 28.315041 \nL 333.563988 27.566281 \nL 334.853664 27.566281 \nL 334.853664 27.191901 \nL 335.498503 27.191901 \nL 335.498503 26.817521 \nL 336.143341 26.817521 \nL 336.143341 25.69438 \nL 338.077855 25.69438 \nL 338.077855 24.94562 \nL 338.722694 24.94562 \nL 338.722694 23.822479 \nL 340.01237 23.822479 \nL 340.01237 23.448099 \nL 340.657208 23.448099 \nL 340.657208 23.073719 \nL 341.302046 23.073719 \nL 341.302046 22.699339 \nL 342.591723 22.699339 \nL 342.591723 22.324959 \nL 343.236561 22.324959 \nL 343.236561 21.950579 \nL 343.881399 21.950579 \nL 343.881399 20.827438 \nL 344.526237 20.827438 \nL 344.526237 20.453058 \nL 345.815914 20.453058 \nL 345.815914 20.078678 \nL 349.040105 20.078678 \nL 349.040105 19.329917 \nL 349.684943 19.329917 \nL 349.684943 17.083636 \nL 349.684943 17.083636 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 37.103125 44.55625 \nL 245.209375 44.55625 \nQ 247.209375 44.55625 247.209375 42.55625 \nL 247.209375 14.2 \nQ 247.209375 12.2 245.209375 12.2 \nL 37.103125 12.2 \nQ 35.103125 12.2 35.103125 14.2 \nL 35.103125 42.55625 \nQ 35.103125 44.55625 37.103125 44.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_15\">\n     <path d=\"M 39.103125 20.298437 \nL 59.103125 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_16\"/>\n    <g id=\"text_13\">\n     <!-- data 1, auc=0.5265688299626907 -->\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 4 \nL 14.015625 -11.625 \nL 7.71875 -11.625 \nL 11.71875 4 \nz\n\" id=\"DejaVuSans-44\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n      <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n      <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n     </defs>\n     <g transform=\"translate(67.103125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"163.964844\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"225.244141\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"257.03125\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"320.654297\" xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"352.441406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"384.228516\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"445.507812\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"508.886719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"563.867188\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"647.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"711.279297\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"743.066406\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"806.689453\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"870.3125\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"933.935547\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"997.558594\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1061.181641\" xlink:href=\"#DejaVuSans-56\"/>\n      <use x=\"1124.804688\" xlink:href=\"#DejaVuSans-56\"/>\n      <use x=\"1188.427734\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1252.050781\" xlink:href=\"#DejaVuSans-57\"/>\n      <use x=\"1315.673828\" xlink:href=\"#DejaVuSans-57\"/>\n      <use x=\"1379.296875\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1442.919922\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"1506.542969\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"1570.166016\" xlink:href=\"#DejaVuSans-57\"/>\n      <use x=\"1633.789062\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1697.412109\" xlink:href=\"#DejaVuSans-55\"/>\n     </g>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 39.103125 34.976562 \nL 59.103125 34.976562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_14\">\n     <!-- data 2, auc=0.5119053351309708 -->\n     <defs>\n      <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n     </defs>\n     <g transform=\"translate(67.103125 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"163.964844\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"225.244141\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"257.03125\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"320.654297\" xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"352.441406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"384.228516\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"445.507812\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"508.886719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"563.867188\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"647.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"711.279297\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"743.066406\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"806.689453\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"870.3125\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"933.935547\" xlink:href=\"#DejaVuSans-57\"/>\n      <use x=\"997.558594\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1061.181641\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1124.804688\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1188.427734\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1252.050781\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1315.673828\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"1379.296875\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"1442.919922\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1506.542969\" xlink:href=\"#DejaVuSans-57\"/>\n      <use x=\"1570.166016\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1633.789062\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1697.412109\" xlink:href=\"#DejaVuSans-56\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pedef220dac\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(0).clf()\n",
    "\n",
    "pred = np.random.rand(1000)\n",
    "label = np.random.randint(2, size=1000)\n",
    "fpr, tpr, thresh = metrics.roc_curve(label, pred)\n",
    "auc = metrics.roc_auc_score(label, pred)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "\n",
    "pred = np.random.rand(1000)\n",
    "label = np.random.randint(2, size=1000)\n",
    "fpr, tpr, thresh = metrics.roc_curve(label, pred)\n",
    "auc = metrics.roc_auc_score(label, pred)\n",
    "plt.plot(fpr,tpr,label=\"data 2, auc=\"+str(auc))\n",
    "\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.misc import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "axial = './data/train/axial/'\n",
    "count = 0\n",
    "\n",
    "for image in os.listdir(axial):\n",
    "    img = os.path.join(axial, image)\n",
    "    img = np.load(img)\n",
    "    for i in range(img.shape[0]):\n",
    "        arr = Image.fromarray(img[i, :, :])\n",
    "        arr.save(f'./images/axial/{image}-{i}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coronal = './data/train/coronal/'\n",
    "count = 0\n",
    "\n",
    "for image in os.listdir(coronal):\n",
    "    img = os.path.join(coronal, image)\n",
    "    img = np.load(img)\n",
    "    for i in range(img.shape[0]):\n",
    "        arr = Image.fromarray(img[i, :, :])\n",
    "        arr.save(f'./images/coronal/{image}-{i}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sagittal = './data/train/sagittal/'\n",
    "count = 0\n",
    "\n",
    "for image in os.listdir(sagittal):\n",
    "    img = os.path.join(sagittal, image)\n",
    "    img = np.load(img)\n",
    "    for i in range(img.shape[0]):\n",
    "        arr = Image.fromarray(img[i, :, :])\n",
    "        arr.save(f'./images/sagittal/{image}-{i}.jpg')"
   ]
  }
 ]
}